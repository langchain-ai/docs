---
title: Common architectures
sidebarTitle: Overview
---

This guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between **workflows** and **agents**: 

- A **workflow** is a predefined sequence of steps or processes that execute in a structured, often linear fashion.
- An **agent** is a system that uses an LLM to dynamically perceive its environment, make decisions, and take actions to achieve goals.

![Agent Workflow](/oss/images/agent_workflow.png)

When building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.

Many LLM applications implement a particular control flow of steps before and/or after LLM calls. However, instead of hard-coding a fixed control flow, it is sometimes beneficial to have LLM systems pick their own control flow to solve more complex problems. There are many ways that an LLM can control an application:

* An LLM can route between two potential paths.
* An LLM can decide which of many tools to call.
* An LLM can decide whether the generated answer is sufficient or more work is needed.

As a result, there are many different types of [agent architectures](https://blog.langchain.dev/what-is-a-cognitive-architecture/), which give an LLM varying levels of control.

![Agent Types](/oss/images/agent_types.png)

## Augmented LLM

LLMs have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on `Building Effective Agents`:

![augmented_llm.png](/oss/images/augmented_llm.png)

```python
# Schema for structured output
from pydantic import BaseModel, Field

class SearchQuery(BaseModel):
    search_query: str = Field(None, description="Query that is optimized web search.")
    justification: str = Field(
        None, description="Why this query is relevant to the user's request."
    )


# Augment the LLM with schema for structured output
structured_llm = llm.with_structured_output(SearchQuery)

# Invoke the augmented LLM
output = structured_llm.invoke("How does Calcium CT score relate to high cholesterol?")

# Define a tool
def multiply(a: int, b: int) -> int:
    return a * b

# Augment the LLM with tools
llm_with_tools = llm.bind_tools([multiply])

# Invoke the LLM with input that triggers the tool call
msg = llm_with_tools.invoke("What is 2 times 3?")

# Get the tool call
msg.tool_calls
```





## Router

A router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from a limited set of pre-defined options. Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.

![routing.png](/oss/images/routing.png)

<Tabs>
  <Tab title="Graph API">
    ```python
    from typing_extensions import Literal
    from langchain_core.messages import HumanMessage, SystemMessage
    
    
    # Schema for structured output to use as routing logic
    class Route(BaseModel):
        step: Literal["poem", "story", "joke"] = Field(
            None, description="The next step in the routing process"
        )
    
    
    # Augment the LLM with schema for structured output
    router = llm.with_structured_output(Route)
    
    
    # State
    class State(TypedDict):
        input: str
        decision: str
        output: str
    
    
    # Nodes
    def llm_call_1(state: State):
        """Write a story"""
    
        result = llm.invoke(state["input"])
        return {"output": result.content}
    
    
    def llm_call_2(state: State):
        """Write a joke"""
    
        result = llm.invoke(state["input"])
        return {"output": result.content}
    
    
    def llm_call_3(state: State):
        """Write a poem"""
    
        result = llm.invoke(state["input"])
        return {"output": result.content}
    
    
    def llm_call_router(state: State):
        """Route the input to the appropriate node"""
    
        # Run the augmented LLM with structured output to serve as routing logic
        decision = router.invoke(
            [
                SystemMessage(
                    content="Route the input to story, joke, or poem based on the user's request."
                ),
                HumanMessage(content=state["input"]),
            ]
        )
    
        return {"decision": decision.step}
    
    
    # Conditional edge function to route to the appropriate node
    def route_decision(state: State):
        # Return the node name you want to visit next
        if state["decision"] == "story":
            return "llm_call_1"
        elif state["decision"] == "joke":
            return "llm_call_2"
        elif state["decision"] == "poem":
            return "llm_call_3"
    
    
    # Build workflow
    router_builder = StateGraph(State)
    
    # Add nodes
    router_builder.add_node("llm_call_1", llm_call_1)
    router_builder.add_node("llm_call_2", llm_call_2)
    router_builder.add_node("llm_call_3", llm_call_3)
    router_builder.add_node("llm_call_router", llm_call_router)
    
    # Add edges to connect nodes
    router_builder.add_edge(START, "llm_call_router")
    router_builder.add_conditional_edges(
        "llm_call_router",
        route_decision,
        {  # Name returned by route_decision : Name of next node to visit
            "llm_call_1": "llm_call_1",
            "llm_call_2": "llm_call_2",
            "llm_call_3": "llm_call_3",
        },
    )
    router_builder.add_edge("llm_call_1", END)
    router_builder.add_edge("llm_call_2", END)
    router_builder.add_edge("llm_call_3", END)
    
    # Compile workflow
    router_workflow = router_builder.compile()
    
    # Show the workflow
    display(Image(router_workflow.get_graph().draw_mermaid_png()))
    
    # Invoke
    state = router_workflow.invoke({"input": "Write me a joke about cats"})
    print(state["output"])
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r
    
    **Resources:**
    
    **LangChain Academy**
    
    See our lesson on routing [here](https://github.com/langchain-ai/langchain-academy/blob/main/module-1/router.ipynb).
    
    **Examples**
    
    See our video [here](https://www.youtube.com/watch?v=bq1Plo2RhYI).

    

  </Tab>
  <Tab title="Functional API">
    ```python
    from typing_extensions import Literal
    from pydantic import BaseModel
    from langchain_core.messages import HumanMessage, SystemMessage
    
    
    # Schema for structured output to use as routing logic
    class Route(BaseModel):
        step: Literal["poem", "story", "joke"] = Field(
            None, description="The next step in the routing process"
        )
    
    
    # Augment the LLM with schema for structured output
    router = llm.with_structured_output(Route)
    
    
    @task
    def llm_call_1(input_: str):
        """Write a story"""
        result = llm.invoke(input_)
        return result.content
    
    
    @task
    def llm_call_2(input_: str):
        """Write a joke"""
        result = llm.invoke(input_)
        return result.content
    
    
    @task
    def llm_call_3(input_: str):
        """Write a poem"""
        result = llm.invoke(input_)
        return result.content
    
    
    def llm_call_router(input_: str):
        """Route the input to the appropriate node"""
        # Run the augmented LLM with structured output to serve as routing logic
        decision = router.invoke(
            [
                SystemMessage(
                    content="Route the input to story, joke, or poem based on the user's request."
                ),
                HumanMessage(content=input_),
            ]
        )
        return decision.step
    
    
    # Create workflow
    @entrypoint()
    def router_workflow(input_: str):
        next_step = llm_call_router(input_)
        if next_step == "story":
            llm_call = llm_call_1
        elif next_step == "joke":
            llm_call = llm_call_2
        elif next_step == "poem":
            llm_call = llm_call_3
    
        return llm_call(input_).result()
    
    # Invoke
    for step in router_workflow.stream("Write me a joke about cats", stream_mode="updates"):
        print(step)
        print("\n")
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r

    

  </Tab>
</Tabs>

### Structured output

Structured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include:

1. Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.
2. Output parsers: Using post-processing to extract structured data from LLM responses.
3. Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.

Structured outputs are crucial for routing as they ensure the LLM's decision can be reliably interpreted and acted upon by the system. Learn more about [structured outputs in this how-to guide](https://python.langchain.com/docs/how_to/structured_output/).

## Tool-calling agent

While a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways:

1. Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.
2. Tool access: The LLM can choose from and use a variety of tools to accomplish tasks.

[ReAct](https://arxiv.org/abs/2210.03629) is a popular general purpose agent architecture that combines these expansions, integrating three core concepts.

1. [Tool calling](#tool-calling): Allowing the LLM to select and use various tools as needed.
2. [Memory](#memory): Enabling the agent to retain and use information from previous steps.
3. [Planning](#planning): Empowering the LLM to create and follow multi-step plans to achieve goals.

This architecture allows for more complex and flexible agent behaviors, going beyond simple routing to enable dynamic problem-solving with multiple steps. Unlike the original [paper](https://arxiv.org/abs/2210.03629), today's agents rely on LLMs' [tool calling](#tool-calling) capabilities and operate on a list of [messages](/oss/python/graph-api#why-use-messages).

In LangGraph, you can use the prebuilt [agent](/oss/python/prebuilts#2-create-an-agent) to get started with tool-calling agents.

### Tool calling

Tools are useful whenever you want an agent to interact with external systems. External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. When we bind an API, for example, as a tool, we give the model awareness of the required input schema. The model will choose to call a tool based upon the natural language input from the user and it will return an output that adheres to the tool's required schema.

[Many LLM providers support tool calling](https://python.langchain.com/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple: you can simply pass any Python `function` into `ChatModel.bind_tools(function)`.

![Tools](/oss/images/tool_call.png)

### Memory

[Memory](/oss/python/add-memory) is crucial for agents, enabling them to retain and utilize information across multiple steps of problem-solving. It operates on different scales:

1. [Short-term memory](/oss/python/add-memory#add-short-term-memory): Allows the agent to access information acquired during earlier steps in a sequence.
2. [Long-term memory](/oss/python/add-memory#add-long-term-memory): Enables the agent to recall information from previous interactions, such as past messages in a conversation.

LangGraph provides full control over memory implementation:

* [`State`](/oss/python/graph-api#state): User-defined schema specifying the exact structure of memory to retain.
* [`Checkpointer`](/oss/python/persistence#checkpoints): Mechanism to store state at every step across different interactions within a session.
* [`Store`](/oss/python/persistence#memory-store): Mechanism to store user-specific or application-level data across sessions.

This flexible approach allows you to tailor the memory system to your specific agent architecture needs. Effective memory management enhances an agent's ability to maintain context, learn from past experiences, and make more informed decisions over time. For a practical guide on adding and managing memory, see [Memory](/oss/python/add-memory).

### Planning

In a tool-calling [agent](/oss/python/agentic-architectures#what-is-an-agent), an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it has enough information to solve the user request and it is not worth calling any more tools.


## Prompt chaining

In prompt chaining, each LLM call processes the output of the previous one.

As noted in the Anthropic blog on `Building Effective Agents`:

> Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see "gate" in the diagram below) on any intermediate steps to ensure that the process is still on track.

> When to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.

![prompt_chain.png](/oss/images/prompt_chain.png)

<Tabs>
  <Tab title="Graph API">
    ```python
    from typing_extensions import TypedDict
    from langgraph.graph import StateGraph, START, END
    from IPython.display import Image, display
    
    
    # Graph state
    class State(TypedDict):
        topic: str
        joke: str
        improved_joke: str
        final_joke: str
    
    
    # Nodes
    def generate_joke(state: State):
        """First LLM call to generate initial joke"""
    
        msg = llm.invoke(f"Write a short joke about {state['topic']}")
        return {"joke": msg.content}
    
    
    def check_punchline(state: State):
        """Gate function to check if the joke has a punchline"""
    
        # Simple check - does the joke contain "?" or "!"
        if "?" in state["joke"] or "!" in state["joke"]:
            return "Pass"
        return "Fail"
    
    
    def improve_joke(state: State):
        """Second LLM call to improve the joke"""
    
        msg = llm.invoke(f"Make this joke funnier by adding wordplay: {state['joke']}")
        return {"improved_joke": msg.content}
    
    
    def polish_joke(state: State):
        """Third LLM call for final polish"""
    
        msg = llm.invoke(f"Add a surprising twist to this joke: {state['improved_joke']}")
        return {"final_joke": msg.content}
    
    
    # Build workflow
    workflow = StateGraph(State)
    
    # Add nodes
    workflow.add_node("generate_joke", generate_joke)
    workflow.add_node("improve_joke", improve_joke)
    workflow.add_node("polish_joke", polish_joke)
    
    # Add edges to connect nodes
    workflow.add_edge(START, "generate_joke")
    workflow.add_conditional_edges(
        "generate_joke", check_punchline, {"Fail": "improve_joke", "Pass": END}
    )
    workflow.add_edge("improve_joke", "polish_joke")
    workflow.add_edge("polish_joke", END)
    
    # Compile
    chain = workflow.compile()
    
    # Show workflow
    display(Image(chain.get_graph().draw_mermaid_png()))
    
    # Invoke
    state = chain.invoke({"topic": "cats"})
    print("Initial joke:")
    print(state["joke"])
    print("\n--- --- ---\n")
    if "improved_joke" in state:
        print("Improved joke:")
        print(state["improved_joke"])
        print("\n--- --- ---\n")
    
        print("Final joke:")
        print(state["final_joke"])
    else:
        print("Joke failed quality gate - no punchline detected!")
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r
    
    **Resources:**
    
    **LangChain Academy**
    
    See our lesson on Prompt Chaining [here](https://github.com/langchain-ai/langchain-academy/blob/main/module-1/chain.ipynb).

    

  </Tab>
  <Tab title="Functional API">
    ```python
    from langgraph.func import entrypoint, task
    
    
    # Tasks
    @task
    def generate_joke(topic: str):
        """First LLM call to generate initial joke"""
        msg = llm.invoke(f"Write a short joke about {topic}")
        return msg.content
    
    
    def check_punchline(joke: str):
        """Gate function to check if the joke has a punchline"""
        # Simple check - does the joke contain "?" or "!"
        if "?" in joke or "!" in joke:
            return "Fail"
    
        return "Pass"
    
    
    @task
    def improve_joke(joke: str):
        """Second LLM call to improve the joke"""
        msg = llm.invoke(f"Make this joke funnier by adding wordplay: {joke}")
        return msg.content
    
    
    @task
    def polish_joke(joke: str):
        """Third LLM call for final polish"""
        msg = llm.invoke(f"Add a surprising twist to this joke: {joke}")
        return msg.content
    
    
    @entrypoint()
    def prompt_chaining_workflow(topic: str):
        original_joke = generate_joke(topic).result()
        if check_punchline(original_joke) == "Pass":
            return original_joke
    
        improved_joke = improve_joke(original_joke).result()
        return polish_joke(improved_joke).result()
    
    # Invoke
    for step in prompt_chaining_workflow.stream("cats", stream_mode="updates"):
        print(step)
        print("\n")
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r

    

  </Tab>
</Tabs>

## Orchestrator-worker

With orchestrator-worker, an orchestrator breaks down a task, delegates each sub-task to workers, and synthesizes their results.

![worker.png](/oss/images/worker.png)

<Tabs>
  <Tab title="Graph API">
    ```python
    from typing import Annotated, List
    import operator
    
    
    # Schema for structured output to use in planning
    class Section(BaseModel):
        name: str = Field(
            description="Name for this section of the report.",
        )
        description: str = Field(
            description="Brief overview of the main topics and concepts to be covered in this section.",
        )
    
    
    class Sections(BaseModel):
        sections: List[Section] = Field(
            description="Sections of the report.",
        )
    
    
    # Augment the LLM with schema for structured output
    planner = llm.with_structured_output(Sections)
    ```
    
    **Creating Workers in LangGraph**
    
    Because orchestrator-worker workflows are common, LangGraph **has the `Send` API to support this**. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a *shared state key* that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and `Send` each to a worker node. See further documentation [here](/oss/python/graph-api#map-reduce-and-the-send-api) and [here](/oss/python/graph-api#send).
    
    ```python
    from langgraph.types import Send
    
    
    # Graph state
    class State(TypedDict):
        topic: str  # Report topic
        sections: list[Section]  # List of report sections
        completed_sections: Annotated[
            list, operator.add
        ]  # All workers write to this key in parallel
        final_report: str  # Final report
    
    
    # Worker state
    class WorkerState(TypedDict):
        section: Section
        completed_sections: Annotated[list, operator.add]
    
    
    # Nodes
    def orchestrator(state: State):
        """Orchestrator that generates a plan for the report"""
    
        # Generate queries
        report_sections = planner.invoke(
            [
                SystemMessage(content="Generate a plan for the report."),
                HumanMessage(content=f"Here is the report topic: {state['topic']}"),
            ]
        )
    
        return {"sections": report_sections.sections}
    
    
    def llm_call(state: WorkerState):
        """Worker writes a section of the report"""
    
        # Generate section
        section = llm.invoke(
            [
                SystemMessage(
                    content="Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting."
                ),
                HumanMessage(
                    content=f"Here is the section name: {state['section'].name} and description: {state['section'].description}"
                ),
            ]
        )
    
        # Write the updated section to completed sections
        return {"completed_sections": [section.content]}
    
    
    def synthesizer(state: State):
        """Synthesize full report from sections"""
    
        # List of completed sections
        completed_sections = state["completed_sections"]
    
        # Format completed section to str to use as context for final sections
        completed_report_sections = "\n\n---\n\n".join(completed_sections)
    
        return {"final_report": completed_report_sections}
    
    
    # Conditional edge function to create llm_call workers that each write a section of the report
    def assign_workers(state: State):
        """Assign a worker to each section in the plan"""
    
        # Kick off section writing in parallel via Send() API
        return [Send("llm_call", {"section": s}) for s in state["sections"]]
    
    
    # Build workflow
    orchestrator_worker_builder = StateGraph(State)
    
    # Add the nodes
    orchestrator_worker_builder.add_node("orchestrator", orchestrator)
    orchestrator_worker_builder.add_node("llm_call", llm_call)
    orchestrator_worker_builder.add_node("synthesizer", synthesizer)
    
    # Add edges to connect nodes
    orchestrator_worker_builder.add_edge(START, "orchestrator")
    orchestrator_worker_builder.add_conditional_edges(
        "orchestrator", assign_workers, ["llm_call"]
    )
    orchestrator_worker_builder.add_edge("llm_call", "synthesizer")
    orchestrator_worker_builder.add_edge("synthesizer", END)
    
    # Compile the workflow
    orchestrator_worker = orchestrator_worker_builder.compile()
    
    # Show the workflow
    display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))
    
    # Invoke
    state = orchestrator_worker.invoke({"topic": "Create a report on LLM scaling laws"})
    
    from IPython.display import Markdown
    Markdown(state["final_report"])
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r
    
    **Resources:**
    
    **LangChain Academy**
    
    See our lesson on orchestrator-worker [here](https://github.com/langchain-ai/langchain-academy/blob/main/module-4/map-reduce.ipynb).
    
    **Examples**
    
    [Here](https://github.com/langchain-ai/report-mAIstro) is a project that uses orchestrator-worker for report planning and writing. See our video [here](https://www.youtube.com/watch?v=wSxZ7yFbbas).

    

  </Tab>
  <Tab title="Functional API">
    ```python
    from typing import List
    
    
    # Schema for structured output to use in planning
    class Section(BaseModel):
        name: str = Field(
            description="Name for this section of the report.",
        )
        description: str = Field(
            description="Brief overview of the main topics and concepts to be covered in this section.",
        )
    
    
    class Sections(BaseModel):
        sections: List[Section] = Field(
            description="Sections of the report.",
        )
    
    
    # Augment the LLM with schema for structured output
    planner = llm.with_structured_output(Sections)
    
    
    @task
    def orchestrator(topic: str):
        """Orchestrator that generates a plan for the report"""
        # Generate queries
        report_sections = planner.invoke(
            [
                SystemMessage(content="Generate a plan for the report."),
                HumanMessage(content=f"Here is the report topic: {topic}"),
            ]
        )
    
        return report_sections.sections
    
    
    @task
    def llm_call(section: Section):
        """Worker writes a section of the report"""
    
        # Generate section
        result = llm.invoke(
            [
                SystemMessage(content="Write a report section."),
                HumanMessage(
                    content=f"Here is the section name: {section.name} and description: {section.description}"
                ),
            ]
        )
    
        # Write the updated section to completed sections
        return result.content
    
    
    @task
    def synthesizer(completed_sections: list[str]):
        """Synthesize full report from sections"""
        final_report = "\n\n---\n\n".join(completed_sections)
        return final_report
    
    
    @entrypoint()
    def orchestrator_worker(topic: str):
        sections = orchestrator(topic).result()
        section_futures = [llm_call(section) for section in sections]
        final_report = synthesizer(
            [section_fut.result() for section_fut in section_futures]
        ).result()
        return final_report
    
    # Invoke
    report = orchestrator_worker.invoke("Create a report on LLM scaling laws")
    from IPython.display import Markdown
    Markdown(report)
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r

    

  </Tab>
</Tabs>

## Evaluator-optimizer

In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:

> When to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.

![evaluator_optimizer.png](/oss/images/evaluator_optimizer.png)

<Tabs>
  <Tab title="Graph API">
    ```python
    # Graph state
    class State(TypedDict):
        joke: str
        topic: str
        feedback: str
        funny_or_not: str
    
    
    # Schema for structured output to use in evaluation
    class Feedback(BaseModel):
        grade: Literal["funny", "not funny"] = Field(
            description="Decide if the joke is funny or not.",
        )
        feedback: str = Field(
            description="If the joke is not funny, provide feedback on how to improve it.",
        )
    
    
    # Augment the LLM with schema for structured output
    evaluator = llm.with_structured_output(Feedback)
    
    
    # Nodes
    def llm_call_generator(state: State):
        """LLM generates a joke"""
    
        if state.get("feedback"):
            msg = llm.invoke(
                f"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}"
            )
        else:
            msg = llm.invoke(f"Write a joke about {state['topic']}")
        return {"joke": msg.content}
    
    
    def llm_call_evaluator(state: State):
        """LLM evaluates the joke"""
    
        grade = evaluator.invoke(f"Grade the joke {state['joke']}")
        return {"funny_or_not": grade.grade, "feedback": grade.feedback}
    
    
    # Conditional edge function to route back to joke generator or end based upon feedback from the evaluator
    def route_joke(state: State):
        """Route back to joke generator or end based upon feedback from the evaluator"""
    
        if state["funny_or_not"] == "funny":
            return "Accepted"
        elif state["funny_or_not"] == "not funny":
            return "Rejected + Feedback"
    
    
    # Build workflow
    optimizer_builder = StateGraph(State)
    
    # Add the nodes
    optimizer_builder.add_node("llm_call_generator", llm_call_generator)
    optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)
    
    # Add edges to connect nodes
    optimizer_builder.add_edge(START, "llm_call_generator")
    optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")
    optimizer_builder.add_conditional_edges(
        "llm_call_evaluator",
        route_joke,
        {  # Name returned by route_joke : Name of next node to visit
            "Accepted": END,
            "Rejected + Feedback": "llm_call_generator",
        },
    )
    
    # Compile the workflow
    optimizer_workflow = optimizer_builder.compile()
    
    # Show the workflow
    display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))
    
    # Invoke
    state = optimizer_workflow.invoke({"topic": "Cats"})
    print(state["joke"])
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r
    
    **Resources:**
    
    **Examples**
    
    [Here](https://github.com/langchain-ai/local-deep-researcher) is an assistant that uses evaluator-optimizer to improve a report. See our video [here](https://www.youtube.com/watch?v=XGuTzHoqlj8).
    
    See our video [here](https://www.youtube.com/watch?v=bq1Plo2RhYI).

    

  </Tab>
  <Tab title="Functional API">
    ```python
    # Schema for structured output to use in evaluation
    class Feedback(BaseModel):
        grade: Literal["funny", "not funny"] = Field(
            description="Decide if the joke is funny or not.",
        )
        feedback: str = Field(
            description="If the joke is not funny, provide feedback on how to improve it.",
        )
    
    
    # Augment the LLM with schema for structured output
    evaluator = llm.with_structured_output(Feedback)
    
    
    # Nodes
    @task
    def llm_call_generator(topic: str, feedback: Feedback):
        """LLM generates a joke"""
        if feedback:
            msg = llm.invoke(
                f"Write a joke about {topic} but take into account the feedback: {feedback}"
            )
        else:
            msg = llm.invoke(f"Write a joke about {topic}")
        return msg.content
    
    
    @task
    def llm_call_evaluator(joke: str):
        """LLM evaluates the joke"""
        feedback = evaluator.invoke(f"Grade the joke {joke}")
        return feedback
    
    
    @entrypoint()
    def optimizer_workflow(topic: str):
        feedback = None
        while True:
            joke = llm_call_generator(topic, feedback).result()
            feedback = llm_call_evaluator(joke).result()
            if feedback.grade == "funny":
                break
    
        return joke
    
    # Invoke
    for step in optimizer_workflow.stream("Cats", stream_mode="updates"):
        print(step)
        print("\n")
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r

    

  </Tab>
</Tabs>

## Human-in-the-loop

Human involvement can significantly enhance agent reliability, especially for sensitive tasks. This can involve:

* Approving specific actions
* Providing feedback to update the agent's state
* Offering guidance in complex decision-making processes

Human-in-the-loop patterns are crucial when full automation isn't feasible or desirable. Learn more in our [human-in-the-loop guide](/oss/python/human-in-the-loop).

## Parallelization 

With parallelization, LLMs work simultaneously and have their outputs aggregated programmatically. Parallel processing is vital for efficient multi-agent systems and complex tasks. LangGraph supports parallelization through its [Send](/oss/python/graph-api#send) API, enabling:

* Concurrent processing of multiple states
* Implementation of map-reduce-like operations
* Efficient handling of independent subtasks

![parallelization.png](/oss/images/parallelization.png)

<Tabs>
  <Tab title="Graph API">
    ```python
    # Graph state
    class State(TypedDict):
        topic: str
        joke: str
        story: str
        poem: str
        combined_output: str
    
    
    # Nodes
    def call_llm_1(state: State):
        """First LLM call to generate initial joke"""
    
        msg = llm.invoke(f"Write a joke about {state['topic']}")
        return {"joke": msg.content}
    
    
    def call_llm_2(state: State):
        """Second LLM call to generate story"""
    
        msg = llm.invoke(f"Write a story about {state['topic']}")
        return {"story": msg.content}
    
    
    def call_llm_3(state: State):
        """Third LLM call to generate poem"""
    
        msg = llm.invoke(f"Write a poem about {state['topic']}")
        return {"poem": msg.content}
    
    
    def aggregator(state: State):
        """Combine the joke and story into a single output"""
    
        combined = f"Here's a story, joke, and poem about {state['topic']}!\n\n"
        combined += f"STORY:\n{state['story']}\n\n"
        combined += f"JOKE:\n{state['joke']}\n\n"
        combined += f"POEM:\n{state['poem']}"
        return {"combined_output": combined}
    
    
    # Build workflow
    parallel_builder = StateGraph(State)
    
    # Add nodes
    parallel_builder.add_node("call_llm_1", call_llm_1)
    parallel_builder.add_node("call_llm_2", call_llm_2)
    parallel_builder.add_node("call_llm_3", call_llm_3)
    parallel_builder.add_node("aggregator", aggregator)
    
    # Add edges to connect nodes
    parallel_builder.add_edge(START, "call_llm_1")
    parallel_builder.add_edge(START, "call_llm_2")
    parallel_builder.add_edge(START, "call_llm_3")
    parallel_builder.add_edge("call_llm_1", "aggregator")
    parallel_builder.add_edge("call_llm_2", "aggregator")
    parallel_builder.add_edge("call_llm_3", "aggregator")
    parallel_builder.add_edge("aggregator", END)
    parallel_workflow = parallel_builder.compile()
    
    # Show workflow
    display(Image(parallel_workflow.get_graph().draw_mermaid_png()))
    
    # Invoke
    state = parallel_workflow.invoke({"topic": "cats"})
    print(state["combined_output"])
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r
    
    **Resources:**
    
    **Documentation**
    
    See our documentation on parallelization [here](/oss/python/graph-api#create-branches).
    
    **LangChain Academy**
    
    See our lesson on parallelization [here](https://github.com/langchain-ai/langchain-academy/blob/main/module-1/simple-graph.ipynb).

    

  </Tab>
  <Tab title="Functional API">
    ```python
    @task
    def call_llm_1(topic: str):
        """First LLM call to generate initial joke"""
        msg = llm.invoke(f"Write a joke about {topic}")
        return msg.content
    
    
    @task
    def call_llm_2(topic: str):
        """Second LLM call to generate story"""
        msg = llm.invoke(f"Write a story about {topic}")
        return msg.content
    
    
    @task
    def call_llm_3(topic):
        """Third LLM call to generate poem"""
        msg = llm.invoke(f"Write a poem about {topic}")
        return msg.content
    
    
    @task
    def aggregator(topic, joke, story, poem):
        """Combine the joke and story into a single output"""
    
        combined = f"Here's a story, joke, and poem about {topic}!\n\n"
        combined += f"STORY:\n{story}\n\n"
        combined += f"JOKE:\n{joke}\n\n"
        combined += f"POEM:\n{poem}"
        return combined
    
    
    # Build workflow
    @entrypoint()
    def parallel_workflow(topic: str):
        joke_fut = call_llm_1(topic)
        story_fut = call_llm_2(topic)
        poem_fut = call_llm_3(topic)
        return aggregator(
            topic, joke_fut.result(), story_fut.result(), poem_fut.result()
        ).result()
    
    # Invoke
    for step in parallel_workflow.stream("cats", stream_mode="updates"):
        print(step)
        print("\n")
    ```
    
    **LangSmith Trace**
    
    https://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r

    

  </Tab>
</Tabs>


## Subgraphs

[Subgraphs](/oss/python/subgraphs) are essential for managing complex agent architectures, particularly in [multi-agent systems](/oss/python/multi-agent). They allow:

* Isolated state management for individual agents
* Hierarchical organization of agent teams
* Controlled communication between agents and the main system

Subgraphs communicate with the parent graph through overlapping keys in the state schema. This enables flexible, modular agent design. For implementation details, refer to our [subgraph how-to guide](/oss/python/subgraphs).

## Reflection

Reflection mechanisms can significantly improve agent reliability by:

1. Evaluating task completion and correctness
2. Providing feedback for iterative improvement
3. Enabling self-correction and learning

While often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in [this video using LangGraph for self-corrective code generation](https://www.youtube.com/watch?v=MvNdgmM7uyc).

By leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.
