---
title: LangGraph quickstart
sidebarTitle: Quickstart
---

This guide shows you how to set up and use LangGraph's **prebuilt**, **reusable** components, which are designed to help you construct agentic systems quickly and reliably.

## Prerequisites

Before you start this tutorial, ensure you have the following:

* An [Anthropic](https://console.anthropic.com/settings/keys) API key

## 1. Install dependencies

If you haven't already, install LangGraph and LangChain:



```bash
npm install @langchain/langgraph @langchain/core @langchain/anthropic
```

<Info>
  LangChain is installed so the agent can call the [model](https://js.langchain.com/docs/integrations/chat/).
</Info>


## 2. Create an agent



To create an agent, use [`createReactAgent`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html):

```typescript
import { ChatAnthropic } from "@langchain/anthropic";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { tool } from "@langchain/core/tools";
import { z } from "zod";

const getWeather = tool(
  // (1)!
  async ({ city }) => {
    return `It's always sunny in ${city}!`;
  },
  {
    name: "get_weather",
    description: "Get weather for a given city.",
    schema: z.object({
      city: z.string().describe("The city to get weather for"),
    }),
  }
);

const agent = createReactAgent({
  llm: new ChatAnthropic({ model: "anthropic:claude-3-5-sonnet-latest" }), // (2)!
  tools: [getWeather], // (3)!
  stateModifier: "You are a helpful assistant", // (4)!
});

// Run the agent
await agent.invoke({
  messages: [{ role: "user", content: "what is the weather in sf" }],
});
```

1. Define a tool for the agent to use. Tools can be defined using the `tool` function. For more advanced tool usage and customization, check the [tools](/oss/javascript/tools) page.
2. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the [models](/oss/javascript/models) page.
3. Provide a list of tools for the model to use.
4. Provide a system prompt (instructions) to the language model used by the agent.


## 3. Configure an LLM



To configure an LLM with specific parameters, such as temperature, use a model instance:

```javascript {highlight={4,6,10}}
import { ChatAnthropic } from "@langchain/anthropic";
import { createReactAgent } from "@langchain/langgraph/prebuilt";

const model = new ChatAnthropic({
  model: "claude-3-5-sonnet-latest",
  temperature: 0,
});

const agent = createReactAgent({
  llm: model,
  tools: [getWeather],
});
```


For more information on how to configure LLMs, see [Models](/oss/javascript/models).

## 4. Add a custom prompt

Prompts instruct the LLM how to behave. Add one of the following types of prompts:

* **Static**: A string is interpreted as a **system message**.
* **Dynamic**: A list of messages generated at **runtime**, based on input or configuration.

<Tabs>
  <Tab title="Static prompt">
    Define a fixed prompt string or list of messages:
    

    
    ```python {highlight={8}}
    import { createReactAgent } from "@langchain/langgraph/prebuilt";
    import { ChatAnthropic } from "@langchain/anthropic";
    
    const agent = createReactAgent({
      llm: new ChatAnthropic({ model: "anthropic:claude-3-5-sonnet-latest" }),
      tools: [getWeather],
      // A static prompt that never changes
      stateModifier: "Never answer questions about the weather."
    });
    
    await agent.invoke({
      messages: [{ role: "user", content: "what is the weather in sf" }]
    });
```

  </Tab>
  <Tab title="Dynamic prompt">

    
    Define a function that returns messages based on the agent's state and configuration:
    
    ```python {highlight={5,14,19}}
    import { type BaseMessageLike } from "@langchain/core/messages";
    import { type RunnableConfig } from "@langchain/core/runnables";
    import { createReactAgent } from "@langchain/langgraph/prebuilt";
    
    const dynamicPrompt = (state: { messages: BaseMessageLike[] }, config: RunnableConfig): BaseMessageLike[] => {  // (1)!
      const userName = config.configurable?.user_name;
      const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`;
      return [{ role: "system", content: systemMsg }, ...state.messages];
    };
    
    const agent = createReactAgent({
      llm: "anthropic:claude-3-5-sonnet-latest",
      tools: [getWeather],
      stateModifier: dynamicPrompt
    });
    
    await agent.invoke(
      { messages: [{ role: "user", content: "what is the weather in sf" }] },
      { configurable: { user_name: "John Smith" } }
    );
```
    
    1. Dynamic prompts allow including non-message [context](/oss/javascript/context) when constructing an input to the LLM, such as:
      * Information passed at runtime, like a `user_id` or API credentials (using `config`).
      * Internal agent state updated during a multi-step reasoning process (using `state`).
      Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM.

  </Tab>
</Tabs>

For more information, see [Context](/oss/javascript/context).

## 5. Add memory

To allow multi-turn conversations with an agent, you need to enable [persistence](/oss/javascript/persistence) by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing `thread_id` â€” a unique identifier for the conversation (session):



```javascript {highlight={4,9,13,16,20}}
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const agent = createReactAgent({
  llm: "anthropic:claude-3-5-sonnet-latest",
  tools: [getWeather],
  checkpointSaver: checkpointer, // (1)!
});

// Run the agent
const config = { configurable: { thread_id: "1" } };
const sfResponse = await agent.invoke(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  config // (2)!
);
const nyResponse = await agent.invoke(
  { messages: [{ role: "user", content: "what about new york?" }] },
  config
);
```

1. `checkpointSaver` allows the agent to store its state at every step in the tool calling loop. This enables [short-term memory](/oss/javascript/add-memory#add-short-term-memory) and [human-in-the-loop](/oss/javascript/human-in-the-loop) capabilities.
2. Pass configuration with `thread_id` to be able to resume the same conversation on future agent invocations.




When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using `MemorySaver`).


Note that in the above example, when the agent is invoked the second time with the same `thread_id`, the original message history from the first conversation is automatically included, together with the new user input.

For more information, see [Memory](/oss/javascript/add-memory).

## 6. Configure structured output



To produce structured responses conforming to a schema, use the `responseFormat` parameter. The schema can be defined with a `Zod` schema. The result will be accessible via the `structuredResponse` field.

```typescript {highlight={11,18}}
import { z } from "zod";
import { createReactAgent } from "@langchain/langgraph/prebuilt";

const WeatherResponse = z.object({
  conditions: z.string(),
});

const agent = createReactAgent({
  llm: "anthropic:claude-3-5-sonnet-latest",
  tools: [getWeather],
  responseFormat: WeatherResponse, // (1)!
});

const response = await agent.invoke({
  messages: [{ role: "user", content: "what is the weather in sf" }],
});

response.structuredResponse;
```

1. When `responseFormat` is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response.
  To provide a system prompt to this LLM, use an object `{ prompt, schema }`, e.g., `responseFormat: { prompt, schema: WeatherResponse }`.


<Note>
  **LLM post-processing**
  Structured output requires an additional call to the LLM to format the response according to the schema.
</Note>

## Next steps

* [Deploy your agent locally](/oss/javascript/local-server)
* [Learn more about prebuilt agents](/oss/javascript/agentic-architectures)
* [LangGraph Platform quickstart](/langgraph-platform/quick-start-studio)
