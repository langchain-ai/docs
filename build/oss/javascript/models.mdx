---
title: Models
---

import ChatModelTabs from '/snippets/chat-model-tabs.mdx';

LangGraph provides built-in support for [LLMs (language models)](https://python.langchain.com/docs/concepts/chat_models/) via the LangChain library. This makes it easy to integrate various LLMs into your agents and workflows.

## Initialize a model



Use model provider classes to initialize models:

<Tabs>
  <Tab title="OpenAI">
    ```typescript
    import { ChatOpenAI } from "@langchain/openai";
    
    const model = new ChatOpenAI({
      model: "gpt-4o",
      temperature: 0,
    });
    ```
  </Tab>
  <Tab title="Anthropic">
    ```typescript
    import { ChatAnthropic } from "@langchain/anthropic";
    
    const model = new ChatAnthropic({
      model: "claude-3-5-sonnet-20240620",
      temperature: 0,
      maxTokens: 2048,
    });
    ```
  </Tab>
  <Tab title="Google">
    ```typescript
    import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
    
    const model = new ChatGoogleGenerativeAI({
      model: "gemini-1.5-pro",
      temperature: 0,
    });
    ```
  </Tab>
  <Tab title="Groq">
    ```typescript
    import { ChatGroq } from "@langchain/groq";
    
    const model = new ChatGroq({
      model: "llama-3.1-70b-versatile",
      temperature: 0,
    });
    ```
  </Tab>
</Tabs>




<Warning>
  **Tool calling support**
  If you are building an agent or workflow that requires the model to call external tools, ensure that the underlying
  language model supports [tool calling](/oss/javascript/tools). Compatible models can be found in the [LangChain integrations directory](https://python.langchain.com/docs/integrations/chat/).
</Warning>

## Use in an agent



When using `createReactAgent` you can pass the model instance directly:

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { createReactAgent } from "@langchain/langgraph/prebuilt";

const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0,
});

const agent = createReactAgent({
  llm: model,
  tools: tools,
});
```




## Advanced model configuration

### Disable streaming



To disable streaming of the individual LLM tokens, set `streaming: false` when initializing the model:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o",
  streaming: false,
});
```


### Add model fallbacks



You can add a fallback to a different model or a different LLM provider using `model.withFallbacks([...])`:

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { ChatAnthropic } from "@langchain/anthropic";

const modelWithFallbacks = new ChatOpenAI({
  model: "gpt-4o",
}).withFallbacks([
  new ChatAnthropic({
    model: "claude-3-5-sonnet-20240620",
  }),
]);
```

See this [guide](https://js.langchain.com/docs/how_to/fallbacks/#fallback-to-better-model) for more information on model fallbacks.




## Bring your own model

If your desired LLM isn't officially supported by LangChain, consider these options:



1. **Implement a custom LangChain chat model**: Create a model conforming to the [LangChain chat model interface](https://js.langchain.com/docs/how_to/custom_chat/). This enables full compatibility with LangGraph's agents and workflows but requires understanding of the LangChain framework.


1. **Direct invocation with custom streaming**: Use your model directly by [adding custom streaming logic](/oss/javascript/streaming#use-with-any-llm) with `StreamWriter`.
  Refer to the [custom streaming documentation](/oss/javascript/streaming#use-with-any-llm) for guidance. This approach suits custom workflows where prebuilt agent integration is not necessary.

## Additional resources



* [Multimodal inputs](https://js.langchain.com/docs/how_to/multimodal_inputs/)
* [Structured outputs](https://js.langchain.com/docs/how_to/structured_output/)
* [Model integration directory](https://js.langchain.com/docs/integrations/chat/)
* [Force model to call a specific tool](https://js.langchain.com/docs/how_to/tool_choice/)
* [All chat model how-to guides](https://js.langchain.com/docs/how_to/#chat-models)
* [Chat model integrations](https://js.langchain.com/docs/integrations/chat/)

