---
title: Get started with LangSmith
sidebarTitle: Overview
---

LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.

![](/langsmith/images/ls-diagram-5be7dd68b135f573a7b0e163692e6800.png)

### [Observability](/langsmith/observability-overview)

Analyze traces in LangSmith and configure metrics, dashboards, alerts based on these.

### [Evals](/langsmith/evaluation-overview)

Evaluate your application over production traffic — score application performance and get human feedback on your data.

### [Prompt Engineering](/langsmith/prompt-engineering-overview)

Iterate on prompts, with automatic version control and collaboration features.

<Check>
  LangSmith is framework-agnostic — it can be used with or without LangChain's open source frameworks [`langchain`](https://python.langchain.com) and [`langgraph`](https://langchain-ai.github.io/langgraph/).

  If you are using either of these, you can enable LangSmith tracing with a single environment variable. For more see the how-to guide for [setting up LangSmith with LangChain](/langsmith/trace-with-langchain) or [setting up LangSmith with LangGraph](https://docs.smith.langchain.com/langsmith/tracing/trace-with-langgraph).
</Check>

## Observability

Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.

This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith’s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.

* Get started by [adding tracing](/langsmith/log-traces-to-project) to your application.
* [Create dashboards](/langsmith/dashboards) to view key metrics like RPS, error rates and costs.

## Evals

The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.

* Get started by [creating your first evaluation](/langsmith/run-evaluation-from-prompt-playground).
* Quickly assess the performance of your application using our [off-the-shelf evaluators](https://docs.smith.langchain.com/langsmith/prebuilt-evaluators) as a starting point.
* [Analyze results](/langsmith/work-with-experiments) of evaluations in the LangSmith UI and [compare results](https://docs.smith.langchain.com/langsmith/compare-experiment-results) over time.
* Easily collect [human feedback](/langsmith/annotation-queues) on your data to improve your application.

## Prompt Engineering

While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.

* Get started by [creating your first prompt](/langsmith/create-a-prompt).
* Iterate on models and prompts using the [Playground](/langsmith/prompt-canvas).
* [Manage prompts programmatically](/langsmith/manage-prompts-programmatically) in your application.
