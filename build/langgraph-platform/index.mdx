---
title: Get started with LangGraph Platform
sidebarTitle: Get started
---

LangGraph Platform is a runtime for deploying and managing long-running, stateful agent workflows in production. It provides APIs for execution, persistence, monitoring, and scaling of agent applications. Agents built with [LangGraph](/oss/python/python/overview) or other frameworks can be hosted on the platform and exposed through managed endpoints.

<Tip>
**Quickstart**  
- [Run locally](/langgraph-platform/local-server) using the LangGraph server process.  
- [Deploy to cloud](/langgraph-platform/deployment-quickstart) for a managed endpoint.
</Tip>

<Columns cols={2}>

  <Card
    title="Deployment Options"
    icon="cloud"
    href="/langgraph-platform/deployment-options"
    arrow="true"
    cta="Learn more"
  >
    Choose from cloud, hybrid, or self-hosted deployments based
    on your infrastructure requirements.
  </Card>

  <Card
    title="LangGraph Studio"
    icon="layout"
    href="/langgraph-platform/langgraph-studio"
    arrow="true"
    cta="Learn more"
  >
    Visualize, debug, and interact with agent workflows.
    Includes integrations with LangSmith for tracing and evaluation.
  </Card>

  <Card
    title="Streaming Support"
    icon="waves"
    href="/langgraph-platform/streaming"
    arrow="true"
    cta="Learn more"
  >
    Stream token outputs and intermediate states back to the client in real time,
    reducing wait times during long operations.
  </Card>

  <Card
    title="Background Runs"
    icon="clock"
    href="/langgraph-platform/background-run"
    arrow="true"
    cta="Learn more"
  >
    Run agents asynchronously for long-duration tasks (minutes to hours),
    with monitoring via polling endpoints or webhooks.
  </Card>

  <Card
    title="Burst Handling"
    icon="server"
    href="/langgraph-platform/background-run"
    arrow="true"
    cta="Learn more"
  >
    Use the built-in task queue to handle bursty request loads without data loss
    or service disruption.
  </Card>

  <Card
    title="Interrupt Handling"
    icon="message-square"
    href="/langgraph-platform/interrupt-concurrent"
    arrow="true"
    cta="Learn more"
  >
    Manage overlapping or rapid user inputs (“double texting”) without breaking
    agent state.
  </Card>

  <Card
    title="Checkpointers & Memory"
    icon="database"
    href="/oss/python/persistence#checkpoints"
    arrow="true"
    cta="Learn more"
  >
    Persist agent state with built-in checkpointing and memory storage,
    removing the need for custom solutions.
  </Card>

  <Card
    title="Human-in-the-Loop"
    icon="user-check"
    href="/langgraph-platform/add-human-in-the-loop"
    arrow="true"
    cta="Learn more"
  >
    Insert human review or intervention into an agent run through dedicated APIs.
  </Card>

</Columns>
