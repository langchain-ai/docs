---
title: "Ollama"
---

> [Ollama](https://ollama.com/) allows you to run open-source large language models, such as [Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/), locally.
>
> `Ollama` bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage. For a complete list of supported models and model variants, see the [Ollama model library](https://ollama.ai/library).

See [this guide](/oss/how_to/local_llms) for more details on how to use `Ollama` with LangChain.

## Installation and Setup

### Ollama installation

Follow [these instructions](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) to set up and run a local Ollama instance.

Ollama will start as a background service automatically, if this is disabled, run:

```
# export OLLAMA_HOST=127.0.0.1 # environment variable to set ollama host# export OLLAMA_PORT=11434 # environment variable to set the ollama portollama serve
```

After starting ollama, run `ollama pull <name-of-model>` to download a model from the [Ollama model library](https://ollama.ai/library):

```
ollama pull llama3.1
```

* This will download the default tagged version of the model. Typically, the default points to the latest, smallest sized-parameter model.
* To view all pulled (downloaded) models, use `ollama list`

We're now ready to install the `langchain-ollama` partner package and run a model.

### Ollama LangChain partner package install

Install the integration package with:

```
pip install langchain-ollama
```

## LLM

```
from langchain_ollama.llms import OllamaLLM
```

**API Reference:**[OllamaLLM](https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html)

See the notebook example [here](/oss/integrations/llms/ollama).

## Chat Models

### Chat Ollama

```
from langchain_ollama.chat_models import ChatOllama
```

**API Reference:**[ChatOllama](https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html)

See the notebook example [here](/oss/integrations/chat/ollama).

### Ollama tool calling

[Ollama tool calling](https://ollama.com/blog/tool-support) uses the OpenAI compatible web server specification, and can be used with the default `BaseChatModel.bind_tools()` methods as described [here](/oss/how_to/tool_calling). Make sure to select an ollama model that supports [tool calling](https://ollama.com/search?\&c=tools).

## Embedding models

```
from langchain_community.embeddings import OllamaEmbeddings
```

**API Reference:**[OllamaEmbeddings](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.ollama.OllamaEmbeddings.html)

See the notebook example [here](/oss/integrations/text_embedding/ollama).
