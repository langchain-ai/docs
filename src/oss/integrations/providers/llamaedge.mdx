---
title: "LlamaEdge"
---

> [LlamaEdge](https://llamaedge.com/oss/intro/) is the easiest & fastest way to run customized and fine-tuned LLMs locally or on the edge.
>
> * Lightweight inference apps. `LlamaEdge` is in MBs instead of GBs
> * Native and GPU accelerated performance
> * Supports many GPU and hardware accelerators
> * Supports many optimized inference libraries
> * Wide selection of AI / LLM models

## Installation and Setup

See the [installation instructions](https://llamaedge.com/oss/user-guide/quick-start-command).

## Chat models

See a [usage example](/oss/integrations/chat/llama_edge).

```
from langchain_community.chat_models.llama_edge import LlamaEdgeChatService
```

**API Reference:**[LlamaEdgeChatService](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.llama_edge.LlamaEdgeChatService.html)
