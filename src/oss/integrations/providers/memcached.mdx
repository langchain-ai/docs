---
title: "Memcached"
---

> [Memcached](https://www.memcached.org/) is a free & open source, high-performance, distributed memory object caching system, generic in nature, but intended for use in speeding up dynamic web applications by alleviating database load.

This page covers how to use Memcached with langchain, using [pymemcache](https://github.com/pinterest/pymemcache) as a client to connect to an already running Memcached instance.

## Installation and Setup

```
pip install pymemcache
```

## LLM Cache

To integrate a Memcached Cache into your application:

```
from langchain.globals import set_llm_cachefrom langchain_openai import OpenAIfrom langchain_community.cache import MemcachedCachefrom pymemcache.client.base import Clientllm = OpenAI(model="gpt-3.5-turbo-instruct", n=2, best_of=2)set_llm_cache(MemcachedCache(Client('localhost')))# The first time, it is not yet in cache, so it should take longerllm.invoke("Which city is the most crowded city in the USA?")# The second time it is, so it goes fasterllm.invoke("Which city is the most crowded city in the USA?")
```

Learn more in the [example notebook](/oss/integrations/llm_caching/#memcached-cache)
