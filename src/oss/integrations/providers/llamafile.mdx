---
title: "llamafile"
---

> [llamafile](https://github.com/Mozilla-Ocho/llamafile) lets you distribute and run LLMs with a single file.

> `llamafile` makes open LLMs much more accessible to both developers and end users. `llamafile` is doing that by combining [llama.cpp](https://github.com/ggerganov/llama.cpp) with [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) into one framework that collapses all the complexity of LLMs down to a single-file executable (called a "llamafile") that runs locally on most computers, with no installation.

## Installation and Setup

See the [installation instructions](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#quickstart).

## LLMs

See a [usage example](/src/oss/integrations/llms/llamafile).

```
from langchain_community.llms.llamafile import Llamafile
```

**API Reference:**[Llamafile](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.llamafile.Llamafile.html)

## Embedding models

See a [usage example](/src/oss/integrations/text_embedding/llamafile).

```
from langchain_community.embeddings import LlamafileEmbeddings
```

**API Reference:**[LlamafileEmbeddings](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.llamafile.LlamafileEmbeddings.html)
