---
title: "MLflow AI Gateway for LLMs"
---

> [The MLflow AI Gateway for LLMs](https://www.mlflow.org/oss/latest/llms/deployments/index.html) is a powerful tool designed to streamline the usage and management of various large language model (LLM) providers, such as OpenAI and Anthropic, within an organization. It offers a high-level interface that simplifies the interaction with these services by providing a unified endpoint to handle specific LLM related requests.

## Installation and Setup

Install `mlflow` with MLflow GenAI dependencies:

```
pip install 'mlflow[genai]'
```

Set the OpenAI API key as an environment variable:

```
export OPENAI_API_KEY=...
```

Create a configuration file:

```
endpoints:  - name: completions    endpoint_type: llm/v1/completions    model:      provider: openai      name: text-davinci-003      config:        openai_api_key: $OPENAI_API_KEY  - name: embeddings    endpoint_type: llm/v1/embeddings    model:      provider: openai      name: text-embedding-ada-002      config:        openai_api_key: $OPENAI_API_KEY
```

Start the gateway server:

```
mlflow gateway start --config-path /path/to/config.yaml
```

## Example provided by `MLflow`

> The `mlflow.langchain` module provides an API for logging and loading `LangChain` models. This module exports multivariate LangChain models in the langchain flavor and univariate LangChain models in the pyfunc flavor.

See the [API documentation and examples](https://www.mlflow.org/oss/latest/llms/langchain/index.html) for more information.

## Completions Example

```
import mlflowfrom langchain.chains import LLMChain, PromptTemplatefrom langchain_community.llms import Mlflowllm = Mlflow(    target_uri="http://127.0.0.1:5000",    endpoint="completions",)llm_chain = LLMChain(    llm=Mlflow,    prompt=PromptTemplate(        input_variables=["adjective"],        template="Tell me a {adjective} joke",    ),)result = llm_chain.run(adjective="funny")print(result)with mlflow.start_run():    model_info = mlflow.langchain.log_model(chain, "model")model = mlflow.pyfunc.load_model(model_info.model_uri)print(model.predict([{"adjective": "funny"}]))
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [Mlflow](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.mlflow.Mlflow.html)

## Embeddings Example

```
from langchain_community.embeddings import MlflowEmbeddingsembeddings = MlflowEmbeddings(    target_uri="http://127.0.0.1:5000",    endpoint="embeddings",)print(embeddings.embed_query("hello"))print(embeddings.embed_documents(["hello"]))
```

**API Reference:**[MlflowEmbeddings](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.mlflow.MlflowEmbeddings.html)

## Chat Example

```
from langchain_community.chat_models import ChatMlflowfrom langchain_core.messages import HumanMessage, SystemMessagechat = ChatMlflow(    target_uri="http://127.0.0.1:5000",    endpoint="chat",)messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French: I love programming."    ),]print(chat(messages))
```

**API Reference:**[ChatMlflow](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.mlflow.ChatMlflow.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html)
