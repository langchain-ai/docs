---
title: "Couchbase"
---

> [Couchbase](http://couchbase.com/) is an award-winning distributed NoSQL cloud database that delivers unmatched versatility, performance, scalability, and financial value for all of your cloud, mobile, AI, and edge computing applications.

## Installation and Setup

We have to install the `langchain-couchbase` package.

```
pip install langchain-couchbase
```

## Vector Store

See a [usage example](/oss/integrations/vectorstores/couchbase).

```
from langchain_couchbase import CouchbaseSearchVectorStoreimport getpass# Constants for the connectionCOUCHBASE_CONNECTION_STRING = getpass.getpass(    "Enter the connection string for the Couchbase cluster: ")DB_USERNAME = getpass.getpass("Enter the username for the Couchbase cluster: ")DB_PASSWORD = getpass.getpass("Enter the password for the Couchbase cluster: ")# Create Couchbase connection objectfrom datetime import timedeltafrom couchbase.auth import PasswordAuthenticatorfrom couchbase.cluster import Clusterfrom couchbase.options import ClusterOptionsauth = PasswordAuthenticator(DB_USERNAME, DB_PASSWORD)options = ClusterOptions(auth)cluster = Cluster(COUCHBASE_CONNECTION_STRING, options)# Wait until the cluster is ready for use.cluster.wait_until_ready(timedelta(seconds=5))vector_store = CouchbaseSearchVectorStore(    cluster=cluster,    bucket_name=BUCKET_NAME,    scope_name=SCOPE_NAME,    collection_name=COLLECTION_NAME,    embedding=my_embeddings,    index_name=SEARCH_INDEX_NAME,)# Add documentstexts = ["Couchbase is a NoSQL database", "LangChain is a framework for LLM applications"]vectorstore.add_texts(texts)# Searchquery = "What is Couchbase?"docs = vectorstore.similarity_search(query)
```

API Reference: [CouchbaseSearchVectorStore](https://couchbase-ecosystem.github.io/langchain-couchbase/langchain_couchbase.html#module-langchain_couchbase.vectorstores.search_vector_store)

## Document loader

See a [usage example](/oss/integrations/document_loaders/couchbase).

```
from langchain_community.document_loaders.couchbase import CouchbaseLoaderconnection_string = "couchbase://localhost"  # valid Couchbase connection stringdb_username = (    "Administrator"  # valid database user with read access to the bucket being queried)db_password = "Password"  # password for the database user# query is a valid SQL++ queryquery = """    SELECT h.* FROM `travel-sample`.inventory.hotel h         WHERE h.country = 'United States'        LIMIT 1        """loader = CouchbaseLoader(    connection_string,    db_username,    db_password,    query,)docs = loader.load()
```

**API Reference:**[CouchbaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.couchbase.CouchbaseLoader.html)

## LLM Caches

### CouchbaseCache

Use Couchbase as a cache for prompts and responses.

See a [usage example](/oss/integrations/llm_caching/#couchbase-caches).

To import this cache:

```
from langchain_couchbase.cache import CouchbaseCache
```

To use this cache with your LLMs:

```
from langchain_core.globals import set_llm_cachecluster = couchbase_cluster_connection_objectset_llm_cache(    CouchbaseCache(        cluster=cluster,        bucket_name=BUCKET_NAME,        scope_name=SCOPE_NAME,        collection_name=COLLECTION_NAME,    ))
```

**API Reference:**[set\_llm\_cache](https://python.langchain.com/api_reference/core/globals/langchain_core.globals.set_llm_cache.html)

API Reference: [CouchbaseCache](https://couchbase-ecosystem.github.io/langchain-couchbase/langchain_couchbase.html#langchain_couchbase.cache.CouchbaseCache)

### CouchbaseSemanticCache

Semantic caching allows users to retrieve cached prompts based on the semantic similarity between the user input and previously cached inputs. Under the hood it uses Couchbase as both a cache and a vectorstore. The CouchbaseSemanticCache needs a Search Index defined to work. Please look at the [usage example](/oss/integrations/vectorstores/couchbase) on how to set up the index.

See a [usage example](/oss/integrations/llm_caching/#couchbase-caches).

To import this cache:

```
from langchain_couchbase.cache import CouchbaseSemanticCache
```

To use this cache with your LLMs:

```
from langchain_core.globals import set_llm_cache# use any embedding provider...from langchain_openai.Embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()cluster = couchbase_cluster_connection_objectset_llm_cache(    CouchbaseSemanticCache(        cluster=cluster,        embedding = embeddings,        bucket_name=BUCKET_NAME,        scope_name=SCOPE_NAME,        collection_name=COLLECTION_NAME,        index_name=INDEX_NAME,    ))
```

**API Reference:**[set\_llm\_cache](https://python.langchain.com/api_reference/core/globals/langchain_core.globals.set_llm_cache.html)

API Reference: [CouchbaseSemanticCache](https://couchbase-ecosystem.github.io/langchain-couchbase/langchain_couchbase.html#langchain_couchbase.cache.CouchbaseSemanticCache)

## Chat Message History

Use Couchbase as the storage for your chat messages.

See a [usage example](/oss/integrations/memory/couchbase_chat_message_history).

To use the chat message history in your applications:

```
from langchain_couchbase.chat_message_histories import CouchbaseChatMessageHistorymessage_history = CouchbaseChatMessageHistory(    cluster=cluster,    bucket_name=BUCKET_NAME,    scope_name=SCOPE_NAME,    collection_name=COLLECTION_NAME,    session_id="test-session",)message_history.add_user_message("hi!")
```

API Reference: [CouchbaseChatMessageHistory](https://couchbase-ecosystem.github.io/langchain-couchbase/langchain_couchbase.html#module-langchain_couchbase.chat_message_histories)
