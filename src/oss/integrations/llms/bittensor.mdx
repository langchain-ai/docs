---
title: "Bittensor"
---

> [Bittensor](https://bittensor.com/) is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.
>
> `NIBittensorLLM` is developed by [Neural Internet](https://neuralinternet.ai/), powered by `Bittensor`.

> This LLM showcases true potential of decentralized AI by giving you the best response(s) from the `Bittensor protocol`, which consist of various AI models such as `OpenAI`, `LLaMA2` etc.

Users can view their logs, requests, and API keys on the [Validator Endpoint Frontend](https://api.neuralinternet.ai/). However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.

If you encounter any difficulties or have any questions, please feel free to reach out to our developer on [GitHub](https://github.com/Kunj-2206), [Discord](https://discordapp.com/users/683542109248159777) or join our discord server for latest update and queries [Neural Internet](https://discord.gg/neuralinternet).

## Different Parameter and response handling for NIBittensorLLM

```
import jsonfrom pprint import pprintfrom langchain.globals import set_debugfrom langchain_community.llms import NIBittensorLLMset_debug(True)# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benefits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrieve top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm.invoke("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)
```

**API Reference:**[set\_debug](https://python.langchain.com/api_reference/langchain/globals/langchain.globals.set_debug.html) | [NIBittensorLLM](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.bittensor.NIBittensorLLM.html)

## Using NIBittensorLLM with LLMChain and PromptTemplate

```
from langchain.chains import LLMChainfrom langchain.globals import set_debugfrom langchain_community.llms import NIBittensorLLMfrom langchain_core.prompts import PromptTemplateset_debug(True)template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate.from_template(template)# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [set\_debug](https://python.langchain.com/api_reference/langchain/globals/langchain.globals.set_debug.html) | [NIBittensorLLM](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.bittensor.NIBittensorLLM.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

## Using NIBittensorLLM with Conversational Agent and Google Search Tool

```
from langchain_community.utilities import GoogleSearchAPIWrapperfrom langchain_core.tools import Toolsearch = GoogleSearchAPIWrapper()tool = Tool(    name="Google Search",    description="Search Google for recent results.",    func=search.run,)
```

**API Reference:**[GoogleSearchAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.google_search.GoogleSearchAPIWrapper.html) | [Tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.simple.Tool.html)

```
from langchain import hubfrom langchain.agents import (    AgentExecutor,    create_react_agent,)from langchain.memory import ConversationBufferMemoryfrom langchain_community.llms import NIBittensorLLMtools = [tool]prompt = hub.pull("hwchase17/react")llm = NIBittensorLLM(    system_prompt="Your task is to determine a response based on user prompt")memory = ConversationBufferMemory(memory_key="chat_history")agent = create_react_agent(llm, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)response = agent_executor.invoke({"input": prompt})
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_react\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [ConversationBufferMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationBufferMemory.html) | [NIBittensorLLM](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.bittensor.NIBittensorLLM.html)

## Related

* LLM [conceptual guide](/src/oss/concepts/text_llms)
* LLM [how-to guides](/src/oss/how_to/#llms)
