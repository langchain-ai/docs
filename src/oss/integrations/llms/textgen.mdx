---
title: "TextGen"
---

[GitHub:oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.

This example goes over how to use LangChain to interact with LLM models via the `text-generation-webui` API integration.

Please ensure that you have `text-generation-webui` configured and an LLM installed. Recommended installation via the [one-click installer appropriate](https://github.com/oobabooga/text-generation-webui#one-click-installers) for your OS.

Once `text-generation-webui` is installed and confirmed working via the web interface, please enable the `api` option either through the web model configuration tab, or by adding the run-time arg `--api` to your start command.

## Set model\_url and run the example

```
model_url = "http://localhost:5000"
```

```
from langchain.chains import LLMChainfrom langchain.globals import set_debugfrom langchain_community.llms import TextGenfrom langchain_core.prompts import PromptTemplateset_debug(True)template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate.from_template(template)llm = TextGen(model_url=model_url)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [set\_debug](https://python.langchain.com/api_reference/langchain/globals/langchain.globals.set_debug.html) | [TextGen](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.textgen.TextGen.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

### Streaming Version

You should install websocket-client to use this feature. `pip install websocket-client`

```
model_url = "ws://localhost:5005"
```

```
from langchain.chains import LLMChainfrom langchain.globals import set_debugfrom langchain_community.llms import TextGenfrom langchain_core.callbacks import StreamingStdOutCallbackHandlerfrom langchain_core.prompts import PromptTemplateset_debug(True)template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate.from_template(template)llm = TextGen(    model_url=model_url, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [set\_debug](https://python.langchain.com/api_reference/langchain/globals/langchain.globals.set_debug.html) | [TextGen](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.textgen.TextGen.html) | [StreamingStdOutCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```
llm = TextGen(model_url=model_url, streaming=True)for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'", stop=["'", "\n"]):    print(chunk, end="", flush=True)
```

## Related

* LLM [conceptual guide](/oss/concepts/text_llms)
* LLM [how-to guides](/oss/how_to/#llms)
