---
title: "Anyscale"
---

[Anyscale](https://www.anyscale.com/) is a fully-managed [Ray](https://www.ray.io/) platform, on which you can build, deploy, and manage scalable AI and Python applications

This example goes over how to use LangChain to interact with [Anyscale Endpoint](https://app.endpoints.anyscale.com/).

```
##Installing the langchain packages needed to use the integration%pip install -qU langchain-community
```

```
ANYSCALE_API_BASE = "..."ANYSCALE_API_KEY = "..."ANYSCALE_MODEL_NAME = "..."
```

```
import osos.environ["ANYSCALE_API_BASE"] = ANYSCALE_API_BASEos.environ["ANYSCALE_API_KEY"] = ANYSCALE_API_KEY
```

```
from langchain.chains import LLMChainfrom langchain_community.llms import Anyscalefrom langchain_core.prompts import PromptTemplate
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [Anyscale](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.anyscale.Anyscale.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```
template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate.from_template(template)
```

```
llm = Anyscale(model_name=ANYSCALE_MODEL_NAME)
```

```
llm_chain = prompt | llm
```

```
question = "When was George Washington president?"llm_chain.invoke({"question": question})
```

With Ray, we can distribute the queries without asynchronized implementation. This not only applies to Anyscale LLM model, but to any other Langchain LLM models which do not have `_acall` or `_agenerate` implemented

```
prompt_list = [    "When was George Washington president?",    "Explain to me the difference between nuclear fission and fusion.",    "Give me a list of 5 science fiction books I should read next.",    "Explain the difference between Spark and Ray.",    "Suggest some fun holiday ideas.",    "Tell a joke.",    "What is 2+2?",    "Explain what is machine learning like I am five years old.",    "Explain what is artifical intelligence.",]
```

```
import ray@ray.remote(num_cpus=0.1)def send_query(llm, prompt):    resp = llm.invoke(prompt)    return respfutures = [send_query.remote(llm, prompt) for prompt in prompt_list]results = ray.get(futures)
```

## Related

* LLM [conceptual guide](/src/oss/concepts/text_llms)
* LLM [how-to guides](/src/oss/how_to/#llms)
