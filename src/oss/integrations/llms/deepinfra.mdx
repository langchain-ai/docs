---
title: "DeepInfra"
---

[DeepInfra](https://deepinfra.com/?utm_source=langchain) is a serverless inference as a service that provides access to a [variety of LLMs](https://deepinfra.com/models?utm_source=langchain) and [embeddings models](https://deepinfra.com/models?type=embeddings\&utm_source=langchain). This notebook goes over how to use LangChain with DeepInfra for language models.

## Set the Environment API Key

Make sure to get your API key from DeepInfra. You have to [Login](https://deepinfra.com/login?from=%2Fdash) and get a new token.

You are given a 1 hour free of serverless GPU compute to test different models. (see [here](https://github.com/deepinfra/deepctl#deepctl)) You can print your token with `deepctl auth token`

```
# get a new token: https://deepinfra.com/login?from=%2Fdashfrom getpass import getpassDEEPINFRA_API_TOKEN = getpass()
```

```
 ········
```

```
import osos.environ["DEEPINFRA_API_TOKEN"] = DEEPINFRA_API_TOKEN
```

## Create the DeepInfra instance

You can also use our open-source [deepctl tool](https://github.com/deepinfra/deepctl#deepctl) to manage your model deployments. You can view a list of available parameters [here](https://deepinfra.com/databricks/dolly-v2-12b#API).

```
from langchain_community.llms import DeepInfrallm = DeepInfra(model_id="meta-llama/Llama-2-70b-chat-hf")llm.model_kwargs = {    "temperature": 0.7,    "repetition_penalty": 1.2,    "max_new_tokens": 250,    "top_p": 0.9,}
```

**API Reference:**[DeepInfra](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.deepinfra.DeepInfra.html)

```
# run inferences directly via wrapperllm("Who let the dogs out?")
```

```
'This is a question that has puzzled many people'
```

```
# run streaming inferencefor chunk in llm.stream("Who let the dogs out?"):    print(chunk)
```

```
 Will Smith.
```

## Create a Prompt Template

We will create a prompt template for Question and Answer.

```
from langchain_core.prompts import PromptTemplatetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate.from_template(template)
```

**API Reference:**[PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

## Initiate the LLMChain

```
from langchain.chains import LLMChainllm_chain = LLMChain(prompt=prompt, llm=llm)
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html)

## Run the LLMChain

Provide a question and run the LLMChain.

```
question = "Can penguins reach the North pole?"llm_chain.run(question)
```

```
"Penguins are found in Antarctica and the surrounding islands, which are located at the southernmost tip of the planet. The North Pole is located at the northernmost tip of the planet, and it would be a long journey for penguins to get there. In fact, penguins don't have the ability to fly or migrate over such long distances. So, no, penguins cannot reach the North Pole. "
```

## Related

* LLM [conceptual guide](/src/oss/concepts/text_llms)
* LLM [how-to guides](/src/oss/how_to/#llms)
