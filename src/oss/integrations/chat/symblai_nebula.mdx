---
title: "Nebula (Symbl.ai)"
---

This notebook covers how to get started with [Nebula](https://docs.symbl.ai/oss/nebula-llm) - Symbl.ai's chat model.

### Integration details

Head to the [API reference](https://docs.symbl.ai/reference/nebula-chat) for detailed documentation.

### Model features: TODO

## Setup

### Credentials

To get started, request a [Nebula API key](https://platform.symbl.ai/#/login) and set the `NEBULA_API_KEY` environment variable:

```
import getpassimport osos.environ["NEBULA_API_KEY"] = getpass.getpass()
```

### Installation

The integration is set up in the `langchain-community` package.

## Instantiation

```
from langchain_community.chat_models.symblai_nebula import ChatNebulafrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage
```

**API Reference:**[ChatNebula](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.symblai_nebula.ChatNebula.html) | [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html)

```
chat = ChatNebula(max_tokens=1024, temperature=0.5)
```

## Invocation

```
messages = [    SystemMessage(        content="You are a helpful assistant that answers general knowledge questions."    ),    HumanMessage(content="What is the capital of France?"),]chat.invoke(messages)
```

```
AIMessage(content=[{'role': 'human', 'text': 'What is the capital of France?'}, {'role': 'assistant', 'text': 'The capital of France is Paris.'}])
```

### Async

```
await chat.ainvoke(messages)
```

```
AIMessage(content=[{'role': 'human', 'text': 'What is the capital of France?'}, {'role': 'assistant', 'text': 'The capital of France is Paris.'}])
```

### Streaming

```
for chunk in chat.stream(messages):    print(chunk.content, end="", flush=True)
```

```
 The capital of France is Paris.
```

### Batch

```
chat.batch([messages])
```

```
[AIMessage(content=[{'role': 'human', 'text': 'What is the capital of France?'}, {'role': 'assistant', 'text': 'The capital of France is Paris.'}])]
```

## Chaining

```
from langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")chain = prompt | chat
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```
chain.invoke({"topic": "cows"})
```

```
AIMessage(content=[{'role': 'human', 'text': 'Tell me a joke about cows'}, {'role': 'assistant', 'text': "Sure, here's a joke about cows:\n\nWhy did the cow cross the road?\n\nTo get to the udder side!"}])
```

## API reference

Check out the [API reference](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.symblai_nebula.ChatNebula.html) for more detail.

## Related

* Chat model [conceptual guide](/src/oss/concepts/chat_models)
* Chat model [how-to guides](/src/oss/how_to/#chat-models)
