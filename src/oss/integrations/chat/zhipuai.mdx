---
title: "ZHIPU AI"
---

This notebook shows how to use [ZHIPU AI API](https://open.bigmodel.cn/dev/api) in LangChain with the langchain.chat\_models.ChatZhipuAI.

> [*GLM-4*](https://open.bigmodel.cn/) is a multi-lingual large language model aligned with human intent, featuring capabilities in Q\&A, multi-turn dialogue, and code generation. The overall performance of the new generation base model GLM-4 has been significantly improved compared to the previous generation, supporting longer contexts; Stronger multimodality; Support faster inference speed, more concurrency, greatly reducing inference costs; Meanwhile, GLM-4 enhances the capabilities of intelligent agents.

## Getting started

### Installation

First, ensure the zhipuai package is installed in your Python environment. Run the following command:

```
#!pip install --upgrade httpx httpx-sse PyJWT
```

### Importing the Required Modules

After installation, import the necessary modules to your Python script:

```
from langchain_community.chat_models import ChatZhipuAIfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage
```

**API Reference:**[ChatZhipuAI](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.zhipuai.ChatZhipuAI.html) | [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html)

### Setting Up Your API Key

Sign in to [ZHIPU AI](https://open.bigmodel.cn/login?redirect=%2Fusercenter%2Fapikeys) for an API Key to access our models.

```
import osos.environ["ZHIPUAI_API_KEY"] = "zhipuai_api_key"
```

### Initialize the ZHIPU AI Chat Model

Here's how to initialize the chat model:

```
chat = ChatZhipuAI(    model="glm-4",    temperature=0.5,)
```

### Basic Usage

Invoke the model with system and human messages like this:

```
messages = [    AIMessage(content="Hi."),    SystemMessage(content="Your role is a poet."),    HumanMessage(content="Write a short poem about AI in four lines."),]
```

```
response = chat.invoke(messages)print(response.content)  # Displays the AI-generated poem
```

## Advanced Features

### Streaming Support

For continuous interaction, use the streaming feature:

```
from langchain_core.callbacks.manager import CallbackManagerfrom langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
```

**API Reference:**[CallbackManager](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManager.html) | [StreamingStdOutCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html)

```
streaming_chat = ChatZhipuAI(    model="glm-4",    temperature=0.5,    streaming=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)
```

```
streaming_chat(messages)
```

### Asynchronous Calls

For non-blocking calls, use the asynchronous approach:

```
async_chat = ChatZhipuAI(    model="glm-4",    temperature=0.5,)
```

```
response = await async_chat.agenerate([messages])print(response)
```

### Using With Functions Call

GLM-4 Model can be used with the function call as wellï¼Œuse the following code to run a simple LangChain json\_chat\_agent.

```
os.environ["TAVILY_API_KEY"] = "tavily_api_key"
```

```
from langchain import hubfrom langchain.agents import AgentExecutor, create_json_chat_agentfrom langchain_community.tools.tavily_search import TavilySearchResultstools = [TavilySearchResults(max_results=1)]prompt = hub.pull("hwchase17/react-chat-json")llm = ChatZhipuAI(temperature=0.01, model="glm-4")agent = create_json_chat_agent(llm, tools, prompt)agent_executor = AgentExecutor(    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_json\_chat\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.json_chat.base.create_json_chat_agent.html) | [TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html)

```
agent_executor.invoke({"input": "what is LangChain?"})
```

## Related

* Chat model [conceptual guide](/src/oss/concepts/chat_models)
* Chat model [how-to guides](/src/oss/how_to/#chat-models)
