---
title: Agent development using prebuilt components
sidebarTitle: Use prebuilt components
---

import prebuiltsTable from '/snippets/prebuilts-table-python.mdx';
import prebuiltsTableJS from '/snippets/prebuilts-table-js.mdx';

LangGraph provides both low-level primitives and high-level prebuilt components for building agent-based applications. This section focuses on the prebuilt, ready-to-use components designed to help you construct agentic systems quickly and reliably—without the need to implement orchestration, memory, or human feedback handling from scratch.

## What is an agent?

An _agent_ consists of three components: a **large language model (LLM)**, a set of **tools** it can use, and a **prompt** that provides instructions.

The LLM operates in a loop. In each iteration, it selects a tool to invoke, provides input, receives the result (an observation), and uses that observation to inform the next action. The loop continues until a stopping condition is met — typically when the agent has gathered enough information to respond to the user.

![Agent loop: the LLM selects tools and uses their outputs to fulfill a user request](/oss/images/agent.png)

## Key features

LangGraph includes several capabilities essential for building robust, production-ready agentic systems:

* [**Memory integration**](/oss/add-memory): Native support for _short-term_ (session-based) and _long-term_ (persistent across sessions) memory, enabling stateful behaviors in chatbots and assistants.
* [**Human-in-the-loop control**](/oss/human-in-the-loop): Execution can pause _indefinitely_ to await human feedback—unlike websocket-based solutions limited to real-time interaction. This enables asynchronous approval, correction, or intervention at any point in the workflow.
* [**Streaming support**](/oss/streaming): Real-time streaming of agent state, model tokens, tool outputs, or combined streams.
* [**Deployment tooling**](/oss/local-server): Includes infrastructure-free deployment tools. [**LangGraph Platform**](/langgraph-platform) supports testing, debugging, and deployment.
  * **[Studio](/langgraph-platform/langgraph-studio)**: A visual IDE for inspecting and debugging workflows.
  * Supports multiple [**deployment options**](/langgraph-platform/deployment-options) for production.

## Package ecosystem

:::python
<prebuiltsTable/>
:::

:::js
<prebuiltsTableJS/>
:::

## Set up

:::python
You can use [any chat model](https://python.langchain.com/docs/integrations/chat/) that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.

Install dependencies

```bash
pip install langchain_core langchain-anthropic langgraph
```

Initialize an LLM

```python
import os
import getpass

from langchain_anthropic import ChatAnthropic

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("ANTHROPIC_API_KEY")

llm = ChatAnthropic(model="claude-3-5-sonnet-latest")
```
:::

:::js
You can use [any chat model](https://js.langchain.com/docs/integrations/chat/) that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.

Install dependencies

```bash
npm install @langchain/core @langchain/anthropic @langchain/langgraph
```

Initialize an LLM

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

process.env.ANTHROPIC_API_KEY = "YOUR_API_KEY";

const llm = new ChatAnthropic({ model: "claude-3-5-sonnet-latest" });
```
:::
