---
title: "Use streaming in a workflow"
sidebarTitle: "Use in a workflow"
---

## Stream outputs

LangGraph graphs expose the `.stream()` (sync) and `.astream()` (async) methods to yield streamed outputs as iterators.

<CodeGroup>

```python Python (Sync)
for chunk in graph.stream(inputs, stream_mode="updates"):
    print(chunk)
```

```python Python (Async)
async for chunk in graph.astream(inputs, stream_mode="updates"):
    print(chunk)
```

</CodeGroup>

### Supported stream modes

| Mode                             | Description                                                                                                                                                                         |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [`values`](#stream-graph-state)  | Streams the full value of the state after each step of the graph.                                                                                                                   |
| [`updates`](#stream-graph-state) | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| [`custom`](#stream-custom-data)  | Streams custom data from inside your graph nodes.                                                                                                                                   |
| [`messages`](#messages)          | Streams LLM tokens and metadata for the graph node where the LLM is invoked.                                                                                                        |
| [`debug`](#debug)                | Streams as much information as possible throughout the execution of the graph.                                                                                                      |


### Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

<CodeGroup>

    ```python Python (Sync)
    for mode, chunk in graph.stream(inputs, stream_mode=["updates", "custom"]):
        print(chunk)
    ```

    ```python Python (Async)
    async for mode, chunk in graph.astream(inputs, stream_mode=["updates", "custom"]):
        print(chunk)
    ```
</CodeGroup>

## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

- `updates` streams the **updates** to the state after each step of the graph.
- `values` streams the **full value** of the state after each step of the graph.

<CodeGroup>

```Python Python
this is a python sample
```

```js JavaScript
this is a js sample
```

</CodeGroup>

## Subgraphs

To include outputs from [subgraphs](../concepts/subgraphs.md) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

```python Python {3}
for chunk in graph.stream(
    {"foo": "foo"},
    subgraphs=True,
    stream_mode="updates",
):
    print(chunk)
```

## Debugging

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

```python Python {3}
for chunk in graph.stream(
    {"topic": "ice cream"},
    stream_mode="debug",
):
    print(chunk)
```

## LLM tokens

Use the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

The streamed output from [`messages` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:

- `message_chunk`: the token or message segment from the LLM.
- `metadata`: a dictionary containing details about the graph node and LLM invocation.

If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.

<Warning>
    When using Python < 3.11 with async code, you must explicitly pass `RunnableConfig` to `ainvoke()` to enable proper streaming. See [Async with Python < 3.11](#async) for details or upgrade to Python 3.11+.
</Warning>

```python Python {17,33}
from dataclasses import dataclass

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START


@dataclass
class MyState:
    topic: str
    joke: str = ""


llm = init_chat_model(model="openai:gpt-4o-mini")

def call_model(state: MyState):
    """Call the LLM to generate a joke about a topic"""
    llm_response = llm.invoke( # (1)!
        [
            {"role": "user", "content": f"Generate a joke about {state.topic}"}
        ]
    )
    return {"joke": llm_response.content}

graph = (
    StateGraph(MyState)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .compile()
)

for message_chunk, metadata in graph.stream( # (2)!
    {"topic": "ice cream"},
    stream_mode="messages",
):
    if message_chunk.content:
        print(message_chunk.content, end="|", flush=True)
```

### Filter by LLM invocation

### Filter by node

## Stream custom data

## Use with any LLM

## Disable streaming

## Async with Python < 3.11


## Streaming with LangGraph Platform

<Info>
The following streaming output options are only available with LangGraph Platform.
</Info>

### Stateless runs


### Join and stream


### Stream events 

_Is this LGP only?_

To stream all events, including the state of the graph:

<CodeGroup>

    ```python Python {5}
    async for chunk in client.runs.stream(
        thread_id,
        assistant_id,
        input={"topic": "ice cream"},
        stream_mode="events"
    ):
        print(chunk.data)
    ```

    ```js JavaScript {6}
    const streamResponse = client.runs.stream(
      threadID,
      assistantID,
      {
        input: { topic: "ice cream" },
        streamMode: "events"
      }
    );
    for await (const chunk of streamResponse) {
      console.log(chunk.data);
    }
    ```

    ```bash curl {7}
    curl --request POST \
    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
    --header 'Content-Type: application/json' \
    --data "{
      \"assistant_id\": \"agent\",
      \"input\": {\"topic\": \"ice cream\"},
      \"stream_mode\": \"events\"
    }"
    ```
</CodeGroup>
